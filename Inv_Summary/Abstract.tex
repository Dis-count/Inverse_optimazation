\documentclass[UTF8]{article}

\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{setspace}
% \usepackage{ctex}
\usepackage{geometry}
\newtheorem{thm}{Theorem}
\newtheorem{pf}{Proof}
\newtheorem{algorithm}{Algorithm}
\numberwithin{equation}{section}
% \geometry{a4paper,left=2cm,right=2cm,top=1cm,bottom=1cm}
% \setstretch{1.5}
\geometry{a4paper,scale=0.8}
\renewcommand{\baselinestretch}{1.5}

\title{Inverse optimization}
% \author{Dis \cdot count}
\date{Jan 2020}
\begin{document}
\maketitle{}

\section{Abstract}
\quad The former inverse optimization models mainly focus on adjusting the parameters of the objective function to make the given feasible solution optimal for the adjusted problem. In this paper, we propose a new method to adjust the left-hand-side matrix of the linear optimization problem by using the lagrangian relaxation to realize the same result in some cases. In this method, we can relax the constraints which we want to modify to the objective function with the corresponding lagrange multipliers. Due to the great quality of zero duality gap on linear optimization problem with lagrangian relaxation, we can obtain the precise adjustment as long as we obtain the optimal lagrange multipliers. In this way, we can adjust the parameters of objective function by the common inverse optimization techniques to achieve the adjustment of the corresponding constraint parameters. Although the model will become the nonlinear optimization problem which is hard to solve after being introduced the lagrange multiplier, we have the following methods to overcome this difficulty. One is that we can transform this model to the convex optimization problem in some cases. Another one is that we can improve the existing subgradient method to obtain the approximate parameters' adjustment of the constraints by iteration.




\end{document}
